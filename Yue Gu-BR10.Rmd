---
title: "Yue Gu-BR10"
author: "Yue Gu"
date: "11/2/2021"
output: html_document
---

```{r}
# Load packages

library(bayesrules) 
library(tidyverse) 
library(bayesplot) 
library(rstanarm)
library(janitor)
library(tidybayes) 
library(broom.mixed)
```

## Exercise 10.1
How fair is the model? How wrong is the model? How accurate are the posterior predictive models?

## Exercise 10.2
(a) When a company collected data of consumers' personal information without gaining consumers' consent in advance or anonymizing consumers' names for commercial analyses, the data collection process violated consumers' privacy, which makes a model unfair.

(b) When a scholar was hired to purposefully collect data in favor of a particular result a company would like to "produce" or promote, e.g. the exploitation of oil resources will not pollute the water nearby, the model is unfair.

(c) When a travel agency collects data from consumers' browsing and shopping histories and use the big data-based analytics to price tour products to the disadvantage of existing customers (same product/service being more expensive for regular customers who are considered to have strong demands for the product/service). This could negatively impact consumers and disrupt the indudtry.

(d) When one tries to use the data on gamers of a specific game to serve the analysis of the whole gamer population, the model is unfair.

## Exercise 10.3
(a) I am Chinese. I grew up in the southeast China (China's regional economic disparity is still great). 

(b) As a Chinese, the fact that I grew up under the governance of a powerful centralized state government may make me, if not in favor of, at least used to a strong authority with great governmental interventions in all sectors and the ideology of collectivism.

The fact that I grew up in a economically developed city in the southeast China makes me lack sufficient knowledge and experience of the true life and status of people from the vast less developed regions in China.

(c) As a Chinese, the fact that I grew up under the governance of a powerful centralized state government may further my understanding of the communist political system, thus enabling me to better analyze and compare different political systems.

The fact that I grew up in an economically developed city in the southeast China makes me more aware of how coastal cities in the modern Chinese history transformed from economies mainly comprised of labor-intensive industries to high-tech economies.

## Exercise 10.4
(a) As human beings living in the society who is constantly exposed to and influenced by different social and cultural norms, beliefs, and artifacts, we inevitably have our own particular mindsets and presumptions that construct how we receive and interpret an event or a message. In any analysis, it’s critical to recognize our own perspective, and that no perspective is natural or neutral. No one would be a neutral observer. What we can do is to realize and prevent potential biases in our data analysis as possible as we can.

(b) Researchers' design of the model based on their selection of the sample, data collection processes, control of relative variables, test of correlations, all contribute to the fairness of a model. There is no perfect or naturally neutral model.

(c) Every model is wrong because a model is a simplification or idealistic representations of more complex realities. But, a model can serve as a means of extracting information of interest from data and an approximation to the reality. If the approximation is close enough, then the model is useful. What’s important to know then is, how wrong is our model? Are our model assumptions reasonable?

## Exercise 10.6
1. Conditioned on X, the observed data Yi on case i is independent of the observed data on any other case j.

2. The typical Y outcome can be written as a linear function of X, μ = β0 + β1X.

3. At any X value, Y varies normally around μ with consistent variability σ.

## Exercise 10.7 
(a) For our first simulated parameter set (−1.8,2.1,0.8), we predict the value of the response variable on day 1 by drawing from the Normal data model evaluated at the observed predictor value X1 on day 1: Y1 | β0，β1，σ ~ N(μ1,(σ1)^2) with μ1=−1.8+2.1*12
Thus Y1 ~ N(23.4,0.64)

(b)
```{r}
data_1 <- matrix(c(12, 10, 4, 8, 6, 20,17,4,11,9), ncol=2, byrow=FALSE)
colnames(data_1) <- c('predictor', 'response_variable')
rownames(data_1) <- seq(1,5, by=1)
df_1 <- as.data.frame(data_1)

df_1
```

```{r}
set.seed(6) 

one_simulation <- df_1  %>%
  mutate(mu = -1.8 + 2.1 * predictor,
         y = response_variable,
         prediction = rnorm(5, mean = mu, sd = 0.8)) %>%
  select(predictor, y, prediction)

head(one_simulation, 5)
```

We can see the predictions and the observed values on day 1, 2, and 5 are relatively close while the difference between the predictions and the observed values on day 3 and 4 are relatively big. Overall, our predictions are larger than the observed values. So our model overestimate the values of the response variable on day 1 to day 5.

## Exercise 10.8
(a) We conduct a posterior predictive check to determine how wrong a model is, namely to what extent do the assumptions behind a model match reality. We can evaluate the model by figuring out how much the predictions generated from the model and observed data are alike.

(b) If posterior predicted sets of n Y values generated from the posterior model have features similar to the original Y data, namely the simulated data captures the big things such as the general center and spread of the original observed data, we have reason to believe that our model assumptions are reasonable; otherwise, our model needs to be changed.

## Exercise 10.9
(a) The median absolute error (MAE) measures the typical difference between the observed data Yi and their posterior predictive means Yi′, the center of the posterior predictive model.

(b) The scaled median absolute error measures the typical number of standard deviations that the observed Yi fall from their posterior predictive means Yi′. It allow us to consider the relative, not just absolute, distance between the observed value and its posterior predictive mean. When different models have similar median absolute error, the scaled median absolute error indicates which model provides the more accurate posterior prediction by standardizing the MAE to show the scales of the errors in different models. Also, when a posterior predictive model is roughly symmetric,  absolute values beyond 2 or 3 on this standardized scale indicate that an observation falls quite far, more than 2 or 3 standard deviations, from its posterior mean prediction (often the observed value falls outside the plausible range of the posterior predictive model).

(c) The within_50 statistic tells the proportion of observed values Yi that fall within their 50% posterior prediction interval of the predictions produced by the posterior predictive model.

## Exercise 10.10
(a) The darker density represents the distribution of the actual observed data. A single lighter-colored density represents the distribution of one posterior dataset simulated with a posterior plausible parameter set in the Markov chain simulation.

(b) If our model fits well, the density plots of the simulated data and observed data will largely coincide, though not exactly the same, the two plots will share similar features in terms of the center, spread, and trends. 

A good fitting model will produce a plot like this since predictions simulated from reasonable model assumptions (though don't exactly replicate all original data features) will capture big things such as the general center and spread, namely the typical value as well as the plausible data range. A good posterior model should be able to simulate a data sample of Y values that is on the whole similar to the original Y data.

(c) If our model fits poorly, the density plots of the simulated data and observed data will appear quite different from each other in terms of key features such as the center, spread, and trends.

## Exercise 10.11 
(a) The ratings of the “anchov-ladas” dish by people with different preferences for anchovies. (The data point we already have is Reem who prefers that anchovies be a part of every meal (10 points in the anchovies preferences) speaks highly of the latest “anchov-ladas” dish among the new taco recipes (giving 100 points to the “anchov-ladas” dish).

(b) Yi|β0,β1, σ ∼ N (μi, σ^2 ) with μi= β0+ β1Xi (Xi denote a person's preference for anchovies in points (1-10), Yi denotes the rating of the “anchov-ladas” dish in points (0-100), μi denotes the local mean rating of “anchov-ladas” dish in points by a certain person, σ measures the variability the rating of “anchov-ladas” dish in points for one person from the local mean for another person with similar preference for anchovies).

(c) Let k be some integer from 2 to my original sample size n. The data points will be split into k non-overlapping subsets, each of equal size. I will then build k separate training models each using the combined data in the first (k − 1) folds, and then tested on the kth data fold, measuring the corresponding prediction quality. Finally, I'll average these k measure metrics of the model prediction quality to obtain a single cross-validation estimate of prediction quality.

(d) Cross-validation would provide a more honest or conservative estimate of how well our model generalizes beyond our particular data sample, i.e., how well it predicts future ratings by people with different tastes of my new taco recipe. I could then see the reception of my latest “anchov-ladas” dish among the general public and make corresponding changes to the recipe. 

## Exercise 10.12
(a)
1. Create folds.

Let k be some integer from 2 to our original sample size n. Split the data into k nonoverlapping folds, or subsets, of roughly equal size.

2. Train and test the model.

Train the model using the combined data in the first k − 1 folds.

Test this model on the k th data fold.

Measure the prediction quality (e.g., by MAE).

3. Repeat.

Repeat step 2 k − 1 more times, each time leaving out a different fold for testing.

4. Calculate cross-validation estimates.

Steps 2 and 3 produce k different training models and k corresponding measures of prediction quality. Average these k measures to obtain a single cross-validation estimate of prediction quality.

(b) A model is optimized to capture the features in the data with which it’s trained or built. Thus, evaluating a model’s predictive accuracy on this same data can produce overly optimistic assessments.

(c) 
```{r}
library(bayesrules)
data("coffee_ratings")
coffee_ratings <- coffee_ratings %>% 
  select(farm_name, total_cup_points, aroma, aftertaste)
```

## Exercise 10.13
(a) In modelling the association between a coffee bean’s rating and its aroma or aftertaste, the data include 1339 batches of beans grown on 571 different farms whose rating (on a 0–100 scale) is respectively graded by features such as its aroma and aftertaste. Naturally, how many points any particular coffee bean grown on one farm scores tells us something about how much another coffee bean from the same farm might scores. Thus, within any given farm, the Y values (coffee beans' ratings) are correlated across multiple data points (coffee beans). The fact that the observed Y values are dependent is likely to violate the independence assumption of the Bayesian linear regression model.

```{r}
head(coffee_ratings)
```

(b)
```{r}
set.seed(84735)
new_coffee <- coffee_ratings %>% 
  group_by(farm_name) %>% 
  sample_n(1) %>% 
  ungroup()
sample_n
dim(new_coffee)
```

## Exercise 10.14 
(a)
```{r}
ggplot(new_coffee, aes(x = aroma, y = total_cup_points)) +
  geom_point(size = 0.5) +
  geom_smooth(method = "lm", se = FALSE)
```

The observed data indicates that the relationship between a coffee bean’s rating and its aroma grade is that a coffee bean’s rating increases with its aroma grade. A coffee bean’s rating tends to be around 82 with an average aroma grade 7.6 but could range from roughly 60 to 90. The variability from the observed patterns reflects some extent of overall uncertainty about the association, but a coffee bean’s rating is strongly associated with its aroma grade and exhibits small variability at any given aroma grade.

```{r}
bean_1 <- na.omit(new_coffee) %>% 
  summarise(mean_aroma = mean(aroma), 
            mean_total_cup_points = mean(total_cup_points),
            std_dev = sd(total_cup_points))

bean_1
```

(b)
```{r}
beans_posterior_default <- stan_glm(
  total_cup_points ~ aroma, data = new_coffee, 
  family = gaussian, 
  prior_intercept = normal(75, 10, autoscale = TRUE), 
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  prior_PD = FALSE,
  chains = 4, iter = 5000*2, seed = 6)
```

(c)
```{r}
# 100 simulated model lines
new_coffee %>%
add_fitted_draws (beans_posterior_default, n = 100) %>% 
  ggplot(aes(x = aroma, y = total_cup_points)) + 
  geom_line(aes(y = .value, group = .draw), alpha = 0.15)+
  geom_point(data = new_coffee, size = 0.05)

# 4 posterior simulated datasets
set.seed(6) 

new_coffee %>%
add_predicted_draws(beans_posterior_default, n = 4) %>% 
  ggplot(aes(x = aroma, y = total_cup_points)) +
  geom_point(aes(y = .prediction, group = .draw)) +
  facet_wrap(~ .draw)
```

```{r}
# Posterior summary statistics
tidy(beans_posterior_default, effects = c("fixed", "aux"), 
     conf.int = TRUE, conf.level = 0.95)
```
```{r}
# Store the 4 chains for each parameter in 1 data frame
beans_posterior_defaultl_df <- as.data.frame(beans_posterior_default)

# Tabulate the beta_1 values that exceed 0
beans_posterior_defaultl_df %>%
  mutate(exceeds_0 = aroma > 0) %>% 
  tabyl(exceeds_0)
```


(d) For every one point increase in a coffee's aroma grade, we expect a coffee's rating to increase by roughly 6 points.

(e) I think we have significant posterior evidence that, the better a coffee bean’s aroma, the higher its rating tends to be. In our visual examination of 100 posterior plausible scenarios for the relationship between a coffee’s rating and its aroma grade, all exhibited positive associations (β1 > 0). A line exhibiting no relationship or negative relationship (β1 = 0 or β1 < 0) would stand out. More rigorously, the 95% posterior credible interval for β1, (5.6, 6.7) lies entirely and well above 0, indicates that this slope could range anywhere between 5.6 and 6.7. We can also do a quick tabulation to approximate that there’s almost certainly a positive association, P (β1 > 0 | y) ≈ 1. All of the 20000 Markov chain values of β1 are positive.

## Exercise 10.15
(a)
```{r}
first_set <- head(beans_posterior_defaultl_df, 1) 
first_set
```

```{r}
beta_0 <- first_set$`(Intercept)` 
beta_1 <- first_set$aroma 
sigma <- first_set$sigma 

set.seed(1996) 

one_simulation <- new_coffee %>%
  mutate(mu = beta_0 + beta_1 * aroma, 
         total_cup_points = total_cup_points,
         simulated_total_cup_points = rnorm(572, mean = mu, sd = sigma)) %>%
  select(aroma, total_cup_points, simulated_total_cup_points)
one_simulation 
```

(b)
```{r}
ggplot(one_simulation, aes(x = simulated_total_cup_points)) +
  geom_density(color = "lightblue") +
  geom_density(aes(x = total_cup_points), color = "darkblue")
```

Though the simulated data doesn’t exactly replicate all original coffee rating features (the observed data distribution is a bit more centralized around the mean than the simulated sample), it does capture the big things such as the general center and general spread. Though the tiny ups and downs in the original data are not reflected very precisely in the simulated data, the simulated data here are generated from merely one of 20,000 posterior plausible parameter sets.

(c)
```{r}
# Examine 572 simulated samples
pp_check(beans_posterior_default, nreps = 572) + 
  xlab("coffee bean ratings")
```

(d) Assumptions 2 and 3 of the Normal regression model are reasonable. The relationship between coffee ratings and aroma does appear to be linear (confirming assumption 2: The typical Y outcome can be written as a linear function of X). In addition, with the slight exception of coffee beans with lower aroma grades of which the ratings are uniformly low, the variability in coffee ratings does appear to be roughly consistent across the range of aroma (confirming assumption 3: At any X value, Y varies normally around μ with consistent variability σ). The density plots further confirms that a Normal regression model of this relationship is good – Though the simulated data doesn’t exactly replicate all original coffee rating features, it does capture the big things such as the general center and spread. The 572 sets of predictions more or less capture the typical coffee rating as well as the observed range in coffee ratings. However, most sets only roughly pick up the twists in the original data while failing to exhibit them precisely. Yet, this doesn’t necessarily mean that our model of coffee ratings is bad – we certainly know more about coffee ratings than we did before. It just means that our model could be better, or less wrong.

```{r}
ggplot(new_coffee, aes(y = total_cup_points, x = aroma)) + 
  geom_point(size = 0.2) + geom_smooth(method = "lm", se = FALSE)
```

## Exercise 10.16
(a) 
```{r}
# Simulate the posterior predictive model
set.seed(16) 

predict_7.67 <- beans_posterior_defaultl_df %>%
  mutate(mu = `(Intercept)` + aroma*7.67, 
       y_new = rnorm(20000, mean = mu, sd = sigma))

predict_7.67

# Plot the posterior predictive model
ggplot(predict_7.67, aes(x = y_new)) + 
  geom_density()
```

(b) 
```{r}
predict_7.67 %>% 
  summarize(mean = mean(y_new), sd = sd(y_new), 
            error = 6228 - mean(y_new), 
            error_scaled = error / sd(y_new))
```

The raw posterior predictive error is 6145.332. The standardized posterior predictive error is 3124.903.

(c)
```{r}
set.seed(6) 

predictions_1 <- posterior_predict(beans_posterior_default, newdata = new_coffee)

dim(predictions_1)
```

```{r}
ppc_intervals(new_coffee$total_cup_points, yrep = predictions_1, x = new_coffee$aroma, 
              prob = 0.5, prob_outer = 0.95)
```

The ppc_intervals() function in the bayesplot package provides a quick visual summary of the 572 approximate posterior predictive models stored in predictions_1 . For each data point in the new_coffee data, ppc_intervals() plots the bounds of the 95% prediction interval (narrow, long blue bars), 50% prediction interval (wider, short blue bars), and posterior predictive median (light blue dots). This information offers a glimpse into the center and spread of each posterior predictive model, as well as the compatibility of each model with the corresponding observed outcome (dark blue dots). In general, almost all of the observed coffee beans' ratings data points fall within the bounds of their 95% prediction interval, and fewer fall within the bounds of their 50% interval. However, the 95% prediction intervals are quite wide – the posterior predictions of a coffee bean's rating with a given aroma grade span a range of nearly 10 points.

(d)
```{r}
# Posterior predictive summaries
set.seed(6) 

prediction_summary(beans_posterior_default, data = new_coffee)
```

```{r}
572*0.6853147
```

About 392 batches have ratings that are within their 50% posterior prediction interval.

## Exercise 10.17
(a)
```{r}
set.seed(6) 

cv_procedure_1 <- prediction_summary_cv( model = beans_posterior_default, data = new_coffee, k = 10)

cv_procedure_1
```

(b)Above are the resulting posterior prediction metrics corresponding to each of the 10 testing folds in this cross-validation procedure. Since the splits are random, the training models perform better on some test sets than on others, essentially depending on how similar the testing data is to the training data. The median absolute error (MAE) was as low as 0.6291223 points for one fold and as high as 1.0702717 for another, but the ultimate cross-validation estimate of the MAE value averaging across each set of 10 MAE values is 0.8892949, which indicates that the typical difference between the observed coffee beans' ratings and their posterior predictive means is 0.8892949, though the difference could plausibly range from 0.6291223 to 1.0702717. 

The scaled median absolute error was as low as 0.3191864 for one fold and as high as 0.5477538 for another, but the ultimate cross-validation estimate of the scaled median absolute error value averaging across each set of 10 scaled median absolute error values is 0.4543068, which indicates that the typical number of standard deviations the observed coffee beans' ratings fall from their posterior predictive means is 0.4543068, though the number could plausibly range from 0.3191864 to 0.5477538.

The within_50 statistic was as low as 0.6034483 for one fold and as high as 0.8421053 for another, but the ultimate cross-validation estimate of the within_50 value averaging across each set of 10 within_50 values is 0.6925287, which indicates that the proportion of observed coffee beans' ratings that fall within their 50% posterior prediction interval of the predictions produced by the posterior predictive model is 0.6925287, though the number could plausibly range from 0.6034483 to 0.8421053.

The within_95 statistic was as low as 0.8947368 for one fold and as high as 1 for another, but the ultimate cross-validation estimate of the within_95 value averaging across each set of 10 within_95 values is 0.9562916, which indicates that the proportion of observed coffee beans' ratings that fall within their 95% posterior prediction interval of the predictions produced by the posterior predictive model is 0.9562916, though the number could plausibly range from 0.8947368 to 1.

(c)
The MAE was as low as 0.6291223 points for one fold and as high as 1.0702717 for another according to the information from the 10 folds. The ultimate cross-validation estimate of the MAE value is the average of the 10 MAE values across the 10 folds, that is 0.8892949.
```{r}
cv_procedure_df <- as.data.frame(cv_procedure_1)  

cv_procedure_df

cv_procedure_df %>% 
  summarize(MAE = mean(folds.mae))
```

## Exercise 10.18
The raw data were human-recorded data using a variety of different encodings, abbreviations, and units of measurement for their farm names, altitude, region, and other fields that were collected from the Coffee Quality Institute's review pages in January 2018 and originally processed by Buzzfeed Data Scientist James LeDoux and distributed through the R for Data Science (2020a). No harm was done and the data have a cleaned version by the time they are distributed to us. The data contain reviews of 1312 arabica and 28 robusta coffee beans from the Coffee Quality Institute's trained reviewers for coffee cupping and were processed and distributed by Buzzfeed Data Scientist James LeDouxs through the R for data science. I can’t imagine how the data collection process or analysis could negatively impact individuals or society. I think there's no harm in collecting data by observing the tastes and aromas of brewed coffee and using the data to practice data science. However, just because I cannot imagine negative societal impacts does not mean that none exist. In any analysis, it’s critical to recognize our own perspective, and that no perspective is natural or neutral. It’s then easier to admit that I might not perceive any negative consequences simply because they do not affect me. To truly determine the impact of our analysis, we’d need to engage and partner with coffee bean farmer/producers who are impacted by the ratings, especially those that have not been part of the decision-making processes. Moreover, we only have data on coffee beans' ratings of 572 randomly sampled coffee beans from different farms/ Thus, our analysis might better inform how to serve this population at the exclusion of serving others. Upon these reflections, I don’t find anything ethically dubious or biased about the data collection or analysis. So I think our coffee bean analysis could be fair.

## Exercise 10.19
(a)
```{r}
coffee_posterior_default <- stan_glm(
  total_cup_points ~ aftertaste, data = new_coffee, 
  family = gaussian, 
  prior_intercept = normal(75, 10, autoscale = TRUE), 
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  prior_PD = FALSE,
  chains = 4, iter = 5000*2, seed = 6)
```

(b)
```{r}
# Examine 572 simulated samples
pp_check(coffee_posterior_default, nreps = 572) + 
  xlab("coffee bean ratings")
```


(c)
```{r}
set.seed(6) 

cv_procedure_2 <- prediction_summary_cv( model = coffee_posterior_default, data = new_coffee, k = 10)

cv_procedure_2
```

(d) Putting it all together, if I could only pick one predictor of coffee bean ratings, I would choose aftertaste since the predictive quality of the model of coffee bean ratings by aftertaste is better than the model using aroma as the predictor given the four cross-validated metrics of the 10-fold cross-validated measurements (the former has smaller ultimate cross-validation estimates of MAE and scaled median absolute error, and larger proportions of observed values Yi that fall within their 50% and 95% posterior prediction intervals).

## Exercise 10.20
(a) 
```{r}
# utilize weakly informative priors
temperature_posterior_default <- stan_glm(
  maxtemp ~ mintemp, data = weather_perth, 
  family = gaussian, 
  prior_intercept = normal(25, 5, autoscale = TRUE), 
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  prior_PD = FALSE,
  chains = 4, iter = 5000*2, seed = 6)
```

(b) 
```{r}
# 100 posterior model lines
weather_1 <- na.omit(weather_perth) %>% 
  add_fitted_draws(temperature_posterior_default, n = 100) %>% 
  ggplot(aes(x = mintemp, y = maxtemp)) + 
  geom_line(aes(y = .value, group = .draw), alpha = 0.15) 

weather_1

# 4 posterior simulated datasets
set.seed(6) 

weather_2 <- na.omit(weather_perth) %>% 
add_predicted_draws(temperature_posterior_default, ndraws = 4) %>% 
  ggplot(aes(x = mintemp, y = maxtemp)) +
  geom_point(aes(y = .prediction, group = .draw)) +
  facet_wrap(~ .draw)

weather_2
```

```{r}
tidy(temperature_posterior_default, effects = c("fixed", "aux"), 
     conf.int = TRUE, conf.level = 0.95)
```

```{r}
# Tabulate the beta_1 values that above 0
temperature_posterior_default_df <- as.data.frame(temperature_posterior_default)
temperature_posterior_default_df %>%
mutate(above_0 = mintemp > 0) %>% 
  tabyl(above_0)
```

My posterior understanding of the relationship between maxtemp and mintemp is that the maximum daily temperature increases with the minimum daily temperature. There’s a positive association between the maximum daily temperature and the minimum daily temperature. In our visual examination of 100 posterior plausible scenarios for the relationship, they all exhibited positive associations. A line exhibiting no relationship or negative relationship (β1 = 0 or β1 < 0) would stand out. The tabulation approximates that there’s almost certainly a positive association, P (β1 > 0 | y) ≈ 1. All of the 20000 Markov chain values of β1 are positive. The posterior median relationship is 14.4723748+0.8354132*X. That is, for every one degree increase in the minimum daily temperature, we expect the maximum daily temperature to increase by roughly 0.8354132 degree. There is posterior uncertainty in this relationship. The 95% posterior credible interval for β1 , (0.7837125, 0.8870247), indicates that this slope lies entirely and well above 0 could and could range anywhere between 0.7837125 and 0.8870247. Above we estimated that σ has a posterior median of 4.2949738 and an 95% credible interval (4.1101740, 4.4879027). Thus, on average, we can expect the observed maximum daily temperature on a given day to fall 4.2949738 degrees from the average maximum daily temperature on days of the same minimum daily temperature. The plots exhibit similarly moderate relationships, indicating relative posterior certainty about the strength in the relationship between the maximum daily temperature and minimum daily temperature. Overall, we are far more confident about the relationship between the maximum daily temperature and minimum daily temperature upon observing some data.

(c)
```{r}
# Examine 1000 simulated samples
pp_check(temperature_posterior_default, nreps = 1000) + 
  xlab("maximum daily temperatures")
```

```{r}
set.seed(84735) 
predictions_3 <- posterior_predict(temperature_posterior_default, newdata = weather_perth) 

dim(predictions_3)
```

```{r}
ppc_intervals(weather_perth$maxtemp, yrep = predictions_3, x = weather_perth$mintemp, prob = 0.5, prob_outer = 0.95)
```

```{r}
set.seed(6) 

cv_procedure_3 <- prediction_summary_cv(model = temperature_posterior_default, data = weather_perth, k = 10)

cv_procedure_3
```

The 1000 sets of predictions shown from the plots well capture the typical maximum daily temperatures as well as the observed range in maximum daily temperatures. However, most sets don’t pick up the apparent bimodality in the original weather data. Yet, this doesn’t necessarily mean that our model of maximum daily temperatures is bad – we certainly know more about the weather than we did before. It just means that it could be better, or less wrong.

The resulting posterior prediction metrics corresponding to each of the 10 testing folds in this cross-validation procedure are also shown. Since the splits are random, the training models perform better on some test sets than on others, essentially depending on how similar the testing data is to the training data. The median absolute error (MAE) was as low as 2.584143 for one fold and as high as 3.551516 for another, but the ultimate cross-validation estimate of the MAE value averaging across each set of 10 MAE values is 3.1557, which indicates that the typical difference between the observed maximum daily temperature and their posterior predictive means is 3.1557, though the difference could plausibly range from 2.584143 to 3.551516.

The scaled median absolute error was as low as 0.5977131 for one fold and as high as 0.8343876 for another, but the ultimate cross-validation estimate of the scaled median absolute error value averaging across each set of 10 scaled median absolute error values is 0.7336017, which indicates that the typical number of standard deviations the observed maximum daily temperature fall from their posterior predictive means is 0.7336017, though the number could plausibly range from 0.5977131 to 0.8343876.

The within_50 statistic was as low as 0.39 for one fold and as high as 0.51 for another, but the ultimate cross-validation estimate of the within_50 value averaging across each set of 10 within_50 values is 0.455, which indicates that the proportion of observed maximum daily temperature that fall within their 50% posterior prediction interval of the predictions produced by the posterior predictive model is 0.455, though the number could plausibly range from 0.39 to 0.51.

The within_95 statistic was as low as 0.89 for one fold and as high as 1 for another, but the ultimate cross-validation estimate of the within_95 value averaging across each set of 10 within_95 values is 0.967, which indicates that the proportion of observed maximum daily temperature that fall within their 95% posterior prediction interval of the predictions produced by the posterior predictive model is 0.967, though the number could plausibly range from 0.89 to 1.

Overall, the predictive quality of the model of the maximum daily temperature by the minimum daily temperature is good since the ultimate cross-validation estimates of MAE and scaled median absolute error are relatively small, and the proportions of observed values Yi that fall within their 50% and 95% posterior prediction intervals are large enough.

## Exercise 10.21
(a) Suppose we have the following prior understanding of this relationship: On an average humidity day, there are typically around 5000 riders, though this average could be somewhere between 1000 and 9000.

Ridership tends to decrease as humidity increases. Specifically, for every one percentage point increase in humidity level, ridership tends to decrease by 10 rides, though this average decrease could be anywhere between 0 and 20.

Ridership is only weakly related to humidity. At any given humidity, ridership will tend to vary with a large standard deviation of 2000 rides.

```{r}
ridership_posterior <- stan_glm(
  rides ~ humidity, data = bikes, 
  family = gaussian, 
  prior_intercept = normal(5000, 2000), 
  prior = normal(-10, 5), 
  prior_aux = exponential(0.0005),
  prior_PD = TRUE,
  chains = 5, iter = 4000*2, seed = 2666)
```

(b)
```{r}
# 100 posterior model lines
bikes %>%
add_fitted_draws (ridership_posterior, n = 100) %>% 
  ggplot(aes(x = humidity, y = rides)) + 
  geom_line(aes(y = .value, group = .draw), alpha = 0.15) 

# 4 posterior simulated datasets
set.seed(6) 

bikes %>%
add_predicted_draws(ridership_posterior, n = 4) %>% 
  ggplot(aes(x = humidity, y = rides)) +
  geom_point(aes(y = .prediction, group = .draw)) +
  facet_wrap(~ .draw)
```

```{r}
tidy(ridership_posterior, effects = c("fixed", "aux"), 
     conf.int = TRUE, conf.level = 0.95)
```

```{r}
# Tabulate the beta_1 values that are below 0
ridership_posterior_df <- as.data.frame(ridership_posterior)
ridership_posterior_df %>%
mutate(below_0 = humidity < 0) %>% 
  tabyl(below_0)
```

My posterior understanding of the relationship between ridership and humidity is that ridership decreases with humidity. There’s a negative association between ridership and humidity. In my visual examination of 100 posterior plausible scenarios for the relationship between ridership and humidity, most of them exhibited negative associations (β1 < 0). The tabulation approximates that there’s almost certainly a negative association, P (β1 < 0 | y) ≈ 1. The posterior median relationship is 5634.14311-10.05539*X. That is, for every one percentage point increase in humidity level, we expect the ridership to decrease by roughly 10. There is posterior uncertainty in this relationship. The 95% posterior credible interval for β1 , (-19.90225, -0.1863978), indicates that this slope lies entirely and well below 0 and could range anywhere between -19.90225 and -0.1863978. The four ridership datasets simulated from the Normal data model using five prior plausible sets of (β0, β1, σ) values also show consistent rate of decrease in ridership with humidity, the baseline ridership, and the variability in ridership with our prior assumptions. Above we estimated that σ has a posterior median of 1406.71888 and an 95% credible interval (54.67924, 7379.3906126). Thus, on average, we can expect the observed ridership on a given day to fall 1406.71888 from the average ridership on days of the similar humidity level, but the deviation could range from 54.67924 to 7379.3906126. The plots exhibit similarly weak relationships, indicating relative posterior uncertainty about the strength in the relationship between the ridership and humidity. Overall, we are more confident about the relationship between the ridership and humidity upon observing some data.

(c) 
```{r}
# Examine 50 of the 20000 simulated samples
pp_check(ridership_posterior, nreps = 50) + 
  xlab("daily ridership")
```

```{r}
set.seed(84735) 
predictions_4 <- posterior_predict(ridership_posterior, newdata = bikes) 

dim(predictions_4)
```

```{r}
ppc_intervals(bikes$rides, yrep = predictions_4, x = bikes$humidity, prob = 0.5, prob_outer = 0.95)
```

```{r}
set.seed(6) 

cv_procedure_4 <- prediction_summary_cv(model = ridership_posterior, data = bikes, k = 10)

cv_procedure_4
```

The 100 sets of predictions shown from the plots do not very well capture the features of the original data, they only vaguely reflect that the typical ridership is around 350 and that the observed range in the ridership is between 0 and 1000. The simulation is not very consistent. Moreover, most sets don’t pick up the humps in the original ridership data. Yet, this doesn’t necessarily mean that our model of ridership is bad – we certainly know more about the ridership than we did before. It just means that it could be better, or less wrong.

The resulting posterior prediction metrics corresponding to each of the 10 testing folds in this cross-validation procedure are also shown. Since the splits are random, the training models perform better on some test sets than on others, essentially depending on how similar the testing data is to the training data. The median absolute error (MAE) was as low as 1415.053 for one fold and as high as 1815.624 for another, but the ultimate cross-validation estimate of the MAE value averaging across each set of 10 MAE values is 1598.788, which indicates that the typical difference between the observed ridership and their posterior predictive means is 1598.788, though the difference could plausibly range from 1415.053 to 1815.624.

The scaled median absolute error was as low as 0.4052391 for one fold and as high as 0.5218093 for another, but the ultimate cross-validation estimate of the scaled median absolute error value averaging across each set of 10 scaled median absolute error values is 0.4602528, which indicates that the typical number of standard deviations the observed ridership fall from their posterior predictive means is 0.4602528, though the number could plausibly range from 0.4052391 to 0.5218093.

The within_50 statistic was as low as 0.50 for one fold and as high as 0.66 for another, but the ultimate cross-validation estimate of the within_50 value averaging across each set of 10 within_50 values is 0.574, which indicates that the proportion of observed ridership that fall within their 50% posterior prediction interval of the predictions produced by the posterior predictive model is 0.574, though the number could plausibly range from 0.50 to 0.66

The within_95 statistic was 1 for each fold, and the ultimate cross-validation estimate of the within_95 value averaging across each set of 10 within_95 values is 1, which indicates that the proportion of observed ridership that fall within their 95% posterior prediction interval of the predictions produced by the posterior predictive model is 1.

Overall, the predictive quality of the model of the ridership by the humidity is acceptable (yet needs improvement) given the ultimate cross-validation estimates of MAE and scaled median absolute error are relatively small, and the proportions of observed values Yi that fall within their 50% and 95% posterior prediction intervals are fairly large.

