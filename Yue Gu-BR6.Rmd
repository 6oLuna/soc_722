---
title: "Yue Gu_BR6"
author: "Yue Gu"
date: "10/13/2021"
output: html_document
---

## Exercise 6.1
(a)
Grid approximation evaluates f(θ | y) at a finite, discrete grid of possible θ values by randomly taking a sample of N independent θ values {θ(1), θ(2), ..., θ(N)} from the discretized pdf to approximate the full posterior pdf f(θ | y). The algorithm evolves in four steps:

1. Define a discrete grid of possible θ values.

2. Evaluate the prior pdf f(θ) and likelihood function L(θ | y) at each θ grid value.

3. Obtain a discrete approximation of the posterior pdf f(θ | y) by: calculating the product f(θ)L(θ | y) at each θ grid value; and then ormalizing the products so that they sum to 1
across all θ.

4. Randomly sample N θ grid values with respect to their corresponding normalized posterior
probabilities.

(b)
I would change step 1 to specify a discrete grid of reasonable values of the parameter of interest using a plot of the prior pdf and likelihood function so that I could see what ranges of values are implausible (though possible). With this in mind, I’ll set up a discrete grid of λ values between a certain range, essentially truncating the posterior’s tail and making the approximation more accurate.

## Exercise 6.2
(a)
```{r}
knitr::include_graphics("image/chain_a.png")
```

(b)
```{r}
knitr::include_graphics("image/chain_b.png")
```

(c)
```{r}
knitr::include_graphics("image/chain_c.png")
```

(d)
```{r}
knitr::include_graphics("image/chain_d.png")
```

## Exercise 6.3
(a) A slow mixing chain with strong trends, small sample size ratio indicates an unstable chain that has not found the range of posterior plausible π values and exhibits strong autocorrelation that drops off very slowly in the Markov chain values which makes them less like independent sampling and generates greater error in the resulting posterior approximation. It would also take a long time for it to adequately explore the full range of the posterior since it moves slowly around the range of posterior plausible values. It may only roughly explored π values in a certain range in its iterations. Thus its posterior approximation will overestimate the plausibility of π values in that range while completely underestimating the plausibility of values outside this range. 

(b) 
Strong correlation between pairs of Markov chain values means strong autocorrelation among the Markov chain values which indicates that the dependence between chain values does not quickly fade away throughout iterations. It indicates that the chain values are less like independent sampling and the chain is mixing slowly. This leads to longer time of simulation, skewed estimate, erroneous approximations of the posterior (as in part a) and thus misleading posterior conclusions.

(c) 
When the chain gets stuck when it visits the certain range(s) of values of π (completely flat lines in the trace plot), the chain will over-sample some values in the corresponding tail of the posterior π values. This phenomenon produces the erroneous spikes in the posterior approximation.

## Exercise 6.4
(a) In practice, we run rstan simulations to approximate the posterior when the models we are interested in analyzing get too complicated to mathematically specify. This means that we won’t have the privilege of being able to compare our simulation results to the “real” posterior which is why diagnostics are important to help us assess our approximations to reach more accurate posterior models and conclusions instead of skewed and erroneous posterior models.

(b) MCMC simulations are helpful in that they provide a more flexible alternative to grid approximation when we work with models that have lots of model parameters. Grid approximation suffers from the curse of dimensionality when simulating multivariate posteriors: It requires dividing the multidimensional sample space of θ = (θ1, θ2, ..., θk) into an extremely fine grid to prevent big gaps in the approximation. In practice, this might not be feasible in terms of time and money. Like grid approximation samples, MCMC samples are not taken directly from the posterior pdf. Though unlike grid approximation samples, MCMC samples are not independent – each subsequent sample value depends directly upon the previous value - with reasonable MCMC algorithms, it mathematically works since the algorithms have a steeper learning curve than the grid approximation technique. 

(c) RStan is one MCMC computing resources that can conduct MCMC simulation for us: It can do the double duty of identifying an appropriate MCMC algorithm for simulating the given model, and then applying this algorithm to the data. Using the rstan package combines the power of R with the Stan engine which can design and run an MCMC algorithm to produce an approximate sample from the posterior.

(d) What don’t you understand about the chapter?
Why does approximating the Normal-Normal model in the rstan syntax indicate the length of the vector following the vector instead of Y as it does in other models like Beta-Binomial and Gamma-Poisson.

Can we produce trace plots for each of the chains separately? label sources of values from different chains, and use ggplot facet 

```{r}
# Load packages
library(tidyverse)
library(janitor)
library(rstan)
library(bayesplot)
library(bayesrules)
library(ggplot2)
```

## Exercise 6.5
(a) 
```{r}
# Step 1: Define a grid of 5 pi values
grid_data_1 <- data.frame(pi_grid_1 = seq(from = 0, to = 1, length = 5))
# Step 2: Evaluate the prior & likelihood at each pi
grid_data_1 <- grid_data_1 %>% 
  mutate(prior_1 = dbeta(pi_grid_1, 3, 8), 
likelihood_1 = dbinom(2, 10, pi_grid_1))
# Step 3: Approximate the posterior
grid_data_1 <- grid_data_1 %>% 
  mutate(unnormalized_1 = likelihood_1 * prior_1,
posterior_1 = unnormalized_1 / sum(unnormalized_1))
# Confirm that the posterior approximation sums to 1
grid_data_1 %>%
summarize(sum(unnormalized_1), sum(posterior_1))
# Examine the grid approximated posterior
round(grid_data_1, 2)
# Plot the grid approximated posterior
ggplot(grid_data_1, aes(x = pi_grid_1, y = posterior_1)) +
geom_point() +
geom_segment(aes(x = pi_grid_1, xend = pi_grid_1, y = 0, yend = posterior_1)) 
set.seed(6)
# Step 4: sample from the discretized posterior
post_sample_1 <- sample_n(grid_data_1, size = 10000,
weight = posterior_1, replace = TRUE)
# A table of the 10000 sample values
post_sample_1 %>% 
  tabyl(pi_grid_1) %>% 
  adorn_totals("row")
# Histogram of the grid simulation with posterior pdf
ggplot(post_sample_1, aes(x = pi_grid_1)) + 
  geom_histogram(aes(y = ..density..), color = "white")+ 
    stat_function(fun = dbeta, args = list(5, 16)) +
lims(x = c(0, 1))
```

(b)
```{r}
# Step 1: Define a grid of 201 pi values
grid_data_2 <- data.frame(pi_grid_2 = seq(from = 0, to = 1, length = 201))
# Step 2: Evaluate the prior & likelihood at each pi
grid_data_2 <- grid_data_2 %>% 
  mutate(prior_2 = dbeta(pi_grid_2, 3, 8), 
likelihood_2 = dbinom(2, 10, pi_grid_2))
# Step 3: Approximate the posterior
grid_data_2 <- grid_data_2 %>% 
  mutate(unnormalized_2 = likelihood_2 * prior_2,
posterior_2 = unnormalized_2 / sum(unnormalized_2))
# Confirm that the posterior approximation sums to 1
grid_data_2 %>%
summarize(sum(unnormalized_2), sum(posterior_2))
# Examine the grid approximated posterior
round(grid_data_2, 2)
# Plot the grid approximated posterior
ggplot(grid_data_2, aes(x = pi_grid_2, y = posterior_2)) +
geom_point() +
geom_segment(aes(x = pi_grid_2, xend = pi_grid_2, y = 0, yend = posterior_2)) 
set.seed(16)
# Step 4: sample from the discretized posterior
post_sample_2 <- sample_n(grid_data_2, size = 10000,
weight = posterior_2, replace = TRUE)
# A table of the 10000 sample values
post_sample_2 %>% 
  tabyl(pi_grid_2) %>% 
  adorn_totals("row")
# Histogram of the grid simulation with posterior pdf
ggplot(post_sample_2, aes(x = pi_grid_2)) + 
  geom_histogram(aes(y = ..density..), color = "white")+ 
    stat_function(fun = dbeta, args = list(5, 16)) +
lims(x = c(0, 1))
```

## Exercise 6.6
(a)
```{r}
# Step 1: Define a grid of 9 lambda values
grid_data_3 <- data.frame(lambda_grid = seq(from = 0, to = 8, length = 9))
# Step 2: Evaluate the prior & likelihood at each lambda
grid_data_3 <- grid_data_3 |> 
  mutate(prior=dgamma(lambda_grid,20,5),
         likelihood=dpois(0,lambda_grid)*dpois(1, lambda_grid)*dpois(0,lambda_grid))
# Step 3: Approximate the posterior
grid_data_3 <- grid_data_3 |> 
  mutate(unnormalized=likelihood*prior, 
         posterior = unnormalized/sum(unnormalized))
# Set the seed
set.seed(616)
# Step 4: sample from the discretized posterior
post_sample_3 <- sample_n(grid_data_3, size = 10000, weight = posterior, replace = TRUE)
# Histogram of the grid simulation with posterior pdf
ggplot(post_sample_3, aes(x = lambda_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white") + 
  stat_function(fun = dgamma, args = list(21, 8)) + 
  lims(x = c(0, 8))
```

(b)
```{r}
# Step 1: Define a grid of 201 lambda values
grid_data_4 <- data.frame(lambda_grid = seq(from = 0, to = 8, length = 201))
# Step 2: Evaluate the prior & likelihood at each lambda
grid_data_4 <- grid_data_4 |> 
  mutate(prior=dgamma(lambda_grid,20,5),
         likelihood=dpois(0,lambda_grid)*dpois(1, lambda_grid)*dpois(0,lambda_grid))
# Step 3: Approximate the posterior
grid_data_4 <- grid_data_4 |> 
  mutate(unnormalized=likelihood*prior, 
         posterior = unnormalized/sum(unnormalized))
# Set the seed
set.seed(616)
# Step 4: sample from the discretized posterior
post_sample_4 <- sample_n(grid_data_4, size = 10000, weight = posterior, replace = TRUE)
# Histogram of the grid simulation with posterior pdf
ggplot(post_sample_4, aes(x = lambda_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white") + 
  stat_function(fun = dgamma, args = list(21, 8)) + 
  lims(x = c(0, 8))
```

## Exercise 6.7
(a)
```{r}
# Step 1: Define a grid of 11 mu values
grid_data_5  <- data.frame(mu_grid = seq(from = 5, to = 15, length = 11))
# Step 2: Evaluate the prior & likelihood at each mu
grid_data_5 <- grid_data_5 %>% 
  mutate(prior = dnorm(mu_grid, mean = 10, sd = 1.2),
likelihood = dnorm(7.1, mean = mu_grid, sd = 1.3)*dnorm(8.9, mean = mu_grid, sd = 1.3)*dnorm(8.4, mean = mu_grid, sd = 1.3)*dnorm(8.6, mean = mu_grid, sd = 1.3))
# Step 3: Approximate the posterior
grid_data_5 <- grid_data_5 %>% 
  mutate(unnormalized = likelihood * prior, 
         posterior = unnormalized / sum(unnormalized))
# Plot the grid approximated posterior
ggplot(grid_data_5, aes(x = mu_grid, y = posterior)) + 
  geom_point() + 
  geom_segment(aes(x = mu_grid, xend = mu_grid, y = 0, yend = posterior))
# Set the seed
set.seed(66)
# Step 4: sample from the discretized posterior
post_sample_5 <- sample_n(grid_data_5, size = 10000, weight = posterior, replace = TRUE)
# Specify actual posterior using formula
posterior_mean_1 <- (((1.3)^2*10+4*(1.2)^2*8.25))/(4*(1.2)^2+(1.3)^2)
posterior_variance_1 <- (1.3)^2*(1.2)^2/(4*(1.2)^2+(1.3)^2)
# Histogram of the grid simulation with posterior pdf
ggplot(post_sample_5, aes(x = mu_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white") + 
  stat_function(fun = dnorm, args = list(posterior_mean_1,posterior_variance_1)) + 
  lims(x = c(5, 15))
```

(b)
```{r}
# Step 1: Define a grid of 201 mu values
grid_data_6  <- data.frame(mu_grid = seq(from = 5, to = 15, length = 201))
# Step 2: Evaluate the prior & likelihood at each mu
grid_data_6 <- grid_data_6 %>% 
  mutate(prior = dnorm(mu_grid, mean = 10, sd = 1.2),
likelihood = dnorm(7.1, mean = mu_grid, sd = 1.3)*dnorm(8.9, mean = mu_grid, sd = 1.3)*dnorm(8.4, mean = mu_grid, sd = 1.3)*dnorm(8.6, mean = mu_grid, sd = 1.3))
# Step 3: Approximate the posterior
grid_data_6 <- grid_data_6 %>% 
  mutate(unnormalized = likelihood * prior, 
         posterior = unnormalized / sum(unnormalized))
# Plot the grid approximated posterior
ggplot(grid_data_6, aes(x = mu_grid, y = posterior)) + 
  geom_point() + 
  geom_segment(aes(x = mu_grid, xend = mu_grid, y = 0, yend = posterior))
# Set the seed
set.seed(66)
# Step 4: sample from the discretized posterior
post_sample_6 <- sample_n(grid_data_6, size = 10000, weight = posterior, replace = TRUE)
# Specify actual posterior using bayersrules
summarize_normal_normal(mean = 10, sd = 1.2, sigma = 1.3, y_bar = 8.25, n = 4)
posterior_mean_2 <- 8.64698
posterior_variance_2 <- 0.3266577
# Histogram of the grid simulation with posterior pdf
ggplot(post_sample_6, aes(x = mu_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white") + 
  stat_function(fun = dnorm, args = list(posterior_mean_2,posterior_variance_2)) + 
  lims(x = c(5, 15))
```

## Exercise 6.8 
(a) To evaluate the U.S national level of support for women's abortion rights requires a model depending on dozens of parameters. Let θ = (θ1, θ2, ..., θk) denote a generic set of k ≥ 1 parameters upon which the Bayesian model depends. In the Bayesian model of "pro-choice" (on abortion) ratio, θ includes 51 parameters that represent the "pro-choice" ratio in each state as well as multiple parameters that define the relationships between support for women's abortion rights among American people, state-level demographics(including religious beliefs), birth rates, feminist movements, etc. Our posterior analysis thus relies on building the posterior pdf of θ given a set of observed data y on state-level polls, demographics, birth rates, feminist movements, etc.

(b) Grid approximation suffers from the curse of dimensionality when simulating multivariate posteriors: It requires dividing the multidimensional sample space of θ = (θ1, θ2, ..., θk) into an extremely fine grid to prevent big gaps in the approximation. In practice, this might not be feasible in terms of time and money.

The increase in unknown dimensions/parameters of interest will bring an exponential grow of the possible sets of parameter values which degrades the performance of a model in searching for optimal parameter values. Adding extra dimensions generally brings in much noise and redundancy, thus leading to more errors. So on one hand, it's ideal to take more dimensions into consideration in many cases, on the other, algorithms are much harder to design in high dimensions and often come at the cost of computational efficiency. 

## Exercise 6.9
(a) Neither of the MCMC or grid approximation takes samples directly from the posterior pdf f(π | y). The result is only a sample that behaves close to a random sample taken directly from f(π | y) itself, thus providing an approximation that is nearly indistinguishable from the real yet intractable posterior. Also, we may not have the opportunity to check the simulation results by comparing it to the “real” posterior in practice.

(b) Both of the MCMC and grid approximations can mimic the real posterior model that is either impossible or prohibitively difficult to specify with sufficient samples, to which the extent can be diagnosed by using visual and numeric measures.

(c) While the grid approximation draws independent samples from a discretized approximation of the posterior pdf f(π | y) that’s defined on a grid of π values, MCMC samples are not independent – each subsequent sample value depends directly upon the previous value. The former is more straightforward.

(d) Grid approximation breaks down in more complicated model settings when we work with models that have lots of model parameters. It suffers from the curse of dimensionality when simulating multivariate posteriors: It requires dividing the multidimensional sample space of θ = (θ1, θ2, ..., θk) into an extremely fine grid to prevent big gaps in the approximation. In practice, this might not be feasible in terms of time and money. MCMC offers a more flexible alternative. Though it produces a dependent sample which are not drawn from the posterior pdf f(θ | y), this sample will mimic the posterior model so long as the chain length N is big enough.

## Exercise 6.10
(a) It is a Markov chain since one's decision on whether to go to the Thai restaurant on day i+1 (i can be any integer bigger than 1 in this case) is mostly dependent on whether he/she went to the Thai restaurant on day i (one decides whether one goes to the restaurant directly based on the most recent experience, e.g., if the last experience was great/improved, one will be more likely to go to the restaurant this time despite the more previous experience; if the last experience was bad/worse than it used to be, one will be less likely to go to the restaurant this time despite the more previous experience. One's mind is updated with the latest experience. So if we know θ(i),then θ(i−1) is of no consequence to θ(i+1) – the only information we need to simulate θ (i+1) is the value of θ (i) ). For instance, say if one did not go to the the Thai restaurant the previous day, there's a 60% chances that he/she will go on day i and a 40% chances that he/she will not; if one did go to the the Thai restaurant the previous day, there's a 70% chances that he/she will go on day i given she had a great experience there and a 30% chances that he/she will not, or it is 30% likely that he/she will go on day i given she had a bad experience there and 70% likely that he/she will not.

(b) It is not a Markov chain since whether one wins the lottery on day i+1 is not dependent on the whether one won the lottery on day i (i can be any positive integer). Theoretically, at least as it is advertised, lottery is based on something that is random, in which case whether one wins the lottery on different days are independent from each other, the previous has no influence on the later probability of winning the lottery.

(c) It is not a Markov chain since whether one wins the game i+1 against one's roommate is independent of whether one won game i against one's roommate (i can be any positive integer). We cannot determine whether one won game i against one's roommate due to his/her better chess skills or just out of luck (one's roommate was sleepy at first, got distracted by something else, or tried to humor one out of courtesy, etc.). Nor can we really determine what the fact that one lost game i indicates. So the fact that whether one won game i cannot give us sufficient information on whether one wins the game i+1. Even if we say there is to some extent a dependency of θ (i+1) on θ(i), whether one wins the game i+1 is not determined by the information that whether one won game i alone. We need to take all the previous θ values into account because they indicate one's chess-playing skills as well as playing form/condition. So in this case we do want to know {θ(1), θ(2),..., θ(i)} to have a picture of one's ability and playing condition and predict θ(+1i). They are all of consequence to the value of θ (i+1).

## Exercise 6.11
(a)
```{r}
# DEFINE the model
bb_model_a <- " 
data {
int<lower = 0, upper = 20> Y; 
}
parameters {
real<lower = 0, upper = 1> pi;
}
model {
Y ~ binomial(20, pi);
pi ~ beta(1, 1); 
}
"
```

(b)
```{r}
# DEFINE the model
gp_model_b <- " 
data {
int<lower = 0> Y[3]; 
}
parameters {
real<lower = 0> lambda;
}
model {
Y ~ poisson(lambda);
lambda ~ gamma(4, 2); 
}
"
```

(c)
```{r}
# DEFINE the model
nn_model_c <- "
data {
    vector[4] Y; // y ∈( − ∞, ∞)
} 
parameters {
    real mu; // μ ∈( − ∞, ∞)
} 
model {
   Y ~ normal(mu, 1);
   mu ~ normal(0, 10);
}
"
```

## Exercise 6.12
(a)
```{r}
# DEFINE the model
bb_model <- " 
data {
int<lower = 0, upper = 20> Y; 
}
parameters {
real<lower = 0, upper = 1> pi;
}
model {
Y ~ binomial(20, pi);
pi ~ beta(1, 1); 
}
"
# SIMULATE the posterior
bb_sim <- stan(model_code = bb_model, data = list(Y = 12), chains = 4, iter = 5000*2, seed = 616)
```

(b)
```{r}
# DEFINE the model
gp_model <- " 
data {
  int<lower = 0> Y[2]; 
  }
parameters {
  real<lower = 0> lambda;
}
model {
  Y ~ poisson(lambda);
  lambda ~ gamma(4, 2); 
  }
"
# SIMULATE the posterior
gp_sim <- stan(model_code = gp_model, data = list(Y = c(1,2)), 
               chains = 4, iter = 5000*2, seed = 6160)
```
(c)
```{r}
# DEFINE the model
nn_model <- "
data {
    vector[4] Y; // y ∈( − ∞, ∞)
} 
parameters {
    real mu; // μ ∈( − ∞, ∞)
} 
model {
   Y ~ normal(mu, 1);
   mu ~ normal(0, 10);
}
"
# SIMULATE the posterior
nn_sim <- stan(model_code = nn_model, data = list(Y = c(3.1, 1.1, 5.4, 2.6)), 
               chains = 4, iter = 5000*2, seed = 616)
```

## Exericise 6.13
(a)
```{r}
bb_model_1 <- "
data {
int<lower = 0, upper = 10> Y;
}
parameters {
real<lower = 0, upper = 1> pi;
}

model {
Y ~ binomial(10, pi);
pi ~ beta(3,8);
}
"

bb_sim_1 <- stan(model_code = bb_model_1, data = list(Y = 2), chains = 3, iter = 12000, seed = 616)
```
(b)
```{r}
mcmc_trace(bb_sim_1, pars = "pi", size = 0.1)
```
(c) The range of values on the trace plot x-axis is the number of iterations that are kept in the simulation sample for each of the three chains. The maximum value of this range is 6000 since, though the total number of iterations being run for each of the three chain is 12000, we toss out the first 6000 iterations of all three chains as burn-in (Without direct knowledge of the posterior it’s trying to simulate, the Markov chain might start out sampling unreasonable values of π. We want to toss the Markov chain values produced during this learning period – keeping them in our sample might lead to a poor posterior approximation), so we end up with three separate Markov chain samples of size 6000.

(d)
```{r}
mcmc_dens_overlay(bb_sim_1, pars = "pi") + 
  ylab("density")
```

(e) Since we know that the prior model of π Beta(α,β) is Beta(3,8) and the observed data Y = 2 for the Bin(n=10, π) model, the Beta(α + y, β + n − y) posterior model follows. Thus the posterior model is Beta(5, 16).
```{r}
plot_beta_binomial(3,8,2,10)
```

Comparing the two plots above, we can see the MCMC approximation is very similar to the real posterior model (Both indicate that π values around 0.22 are the most likely and the plausible range of π values is between 0 and approximately 0.625).

## Exercise 6.14
(a)
```{r}
bb_model_2 <- "
data {
int<lower = 0, upper = 12> Y;
}
parameters {
real<lower = 0, upper = 1> pi;
}

model {
Y ~ binomial(12, pi);
pi ~ beta(4,3);
}
"

bb_sim_2 <- stan(model_code = bb_model_2, data = list(Y = 4), chains = 3, iter = 12000, seed = 616)
```

(b)
```{r}
mcmc_trace(bb_sim_2, pars = "pi", size = 0.1)
```

(c) The range of values on the trace plot x-axis is the number of iterations that are kept in the simulation sample for each of the three chains. The maximum value of this range is 6000 since, though the total number of iterations being run for each of the three chain is 12000, we toss out the first 6000 iterations of all three chains as burn-in (Without direct knowledge of the posterior it’s trying to simulate, the Markov chain might start out sampling unreasonable values of π. We want to toss the Markov chain values produced during this learning period – keeping them in our sample might lead to a poor posterior approximation), so we end up with three separate Markov chain samples of size 6000.

(d)
```{r}
mcmc_dens_overlay(bb_sim_2, pars = "pi") + 
  ylab("density")
```

(e) Since we know that the prior model of π Beta(α,β) is Beta(4,3) and the observed data Y = 4 for the Bin(n=12, π) model, the Beta(α + y, β + n − y) posterior model follows. Thus the posterior model is Beta(8, 11).
```{r}
plot_beta_binomial(4,3,4,12)
```

Comparing the two plots above, we can see the MCMC approximation is very similar to the real posterior model (Both indicate that π values around 0.4 are the most likely and the plausible range of π values is between 0 and approximately 0.8).

## Exercise 6.15 
(a)
```{r}
gp_model_1 <- " 
data {
  int<lower = 0> Y[3]; 
  }
parameters {
  real<lower = 0> lambda;
}
model {
  Y ~ poisson(lambda);
  lambda ~ gamma(20, 5); 
  }
"

gp_sim_1 <- stan(model_code = gp_model_1, data = list(Y = c(0,1,0)), 
               chains = 4, iter = 10000, seed = 616)
```

(b)
```{r}
mcmc_trace(gp_sim_1, pars = "lambda", size = 0.1)
mcmc_dens_overlay(gp_sim_1, pars = "lambda") + 
  yaxis_text(TRUE) +
ylab("density")
```

(c)From the density plots, 2.5 seems to be the most posterior plausible value of λ.

(d) The Gamma-Poisson Bayesian model complements the Poisson structure of data Y with a Gamma prior on λ. Since we know the prior model of λ Gamma(s,r) is Gamma(20,5) and the n=3 independent data points (Y1,Y2,Y3)=(0,1,0), the posterior model Gamma (s+∑y(i), r+n) follows to be Gamma(21,8).

```{r}
plot_gamma_poisson(20,5,1,3)
```

Comparing the two plots above, we can see the MCMC approximation is very similar to the real posterior model (Both indicate that λ values around 2.5 are the most likely and the plausible range of λ values is between 0 and approximately 5).

## Exercise 6.16
(a)
```{r}
gp_model_2 <- " 
data {
  int<lower = 0> Y[3]; 
  }
parameters {
  real<lower = 0> lambda;
}
model {
  Y ~ poisson(lambda);
  lambda ~ gamma(5, 5); 
  }
"

gp_sim_2 <- stan(model_code = gp_model_2, data = list(Y = c(0,1,0)), 
               chains = 4, iter = 10000, seed = 616)
```

(b)
```{r}
mcmc_trace(gp_sim_2, pars = "lambda", size = 0.1)
mcmc_dens_overlay(gp_sim_2, pars = "lambda") + 
  yaxis_text(TRUE) +
ylab("density")
```

(c)From the density plots, 0.65 seems to be the most posterior plausible value of λ.

(d) The Gamma-Poisson Bayesian model complements the Poisson structure of data Y with a Gamma prior on λ. Since we know the prior model of λ Gamma(s,r) is Gamma(5,5) and the n=3 independent data points (Y1,Y2,Y3)=(0,1,0), the posterior model Gamma (s+∑y(i), r+n) follows to be Gamma(6,8).

```{r}
plot_gamma_poisson(5,5,1,3)
```

Comparing the two plots above, we can see the MCMC approximation is very similar to the real posterior model (Both indicate that λ values around 0.65 are the most likely and the plausible range of λ values is between 0 and approximately 2.5).

## Exercise 6.17
(a)
```{r}
nn_model_1 <- "
data {
    vector[4] Y;
} 
parameters {
    real mu;
} 
model {
   Y ~ normal(mu, 1.3);
   mu ~ normal(10, 1.2);
}
"
nn_sim_1 <- stan(model_code = nn_model_1, data = list(Y = c(7.1,8.9,8.4,8.6)), 
               chains = 4, iter = 10000, seed = 606)
```

(b)
```{r}
mcmc_trace(nn_sim_1, pars = "mu", size = 0.1)
mcmc_dens_overlay(nn_sim_1, pars = "mu") + 
  yaxis_text(TRUE) +
ylab("density")
```

(c)From the density plots, 8.65 seems to be the most posterior plausible value of μ.

(d) The Normal-Normal Bayesian model complements the Normal data structure with a Normal prior on μ. Since we know the prior model of μ N(μ, σ^2) is N(10,1.2*1.2) with Y(i)|μ ∼ N(μ,1.3*1.3) and n=4 independent observations of data (Y1,Y2,Y3,Y4)=(7.1,8.9,8.4,8.6), the posterior model is N(27.78255, 0.3266577).

```{r}
summarize_normal_normal(mean = 10, sd = 1.2, sigma = 1.3, y_bar = 8.25, n = 4)
```
```{r}
plot_normal_normal(10,1.2,1.3,8.25,4)
```

Comparing the two plots above, we can see the MCMC approximation is very similar to the real posterior model (Both indicate that μ values around 8.65 are the most likely and the plausible range of μ values is between 0 and approximately 11.25).

## Exercise 6.18
(a)
```{r}
nn_model_2 <- "
data {
    vector[5] Y;
} 
parameters {
    real mu;
} 
model {
   Y ~ normal(mu, 8);
   mu ~ normal(-14, 2);
}
"
nn_sim_2 <- stan(model_code = nn_model_2, data = list(Y = c(-10.1,5.5,0.1,-1.4,11.5)), 
               chains = 4, iter = 10000, seed = 606)
```

(b)
```{r}
mcmc_trace(nn_sim_2, pars = "mu", size = 0.1)
mcmc_dens_overlay(nn_sim_2, pars = "mu") + 
  yaxis_text(TRUE) +
ylab("density")
```

(c)From the density plots, -10 seems to be the most posterior plausible value of μ.

(d) The Normal-Normal Bayesian model complements the Normal data structure with a Normal prior on μ. Since we know the prior model of μ N(μ, σ^2) is N(-14, 2^2) with Y(i)|μ ∼ N(μ, 8^2) and n=4 independent observations of data (Y1,Y2,Y3,Y4,Y5)=(-10.1,5.5,0.1,-1.4,11.5), the posterior model is N(-10.4, 3.047619).

```{r}
summarize_normal_normal(mean = -14, sd = 2, sigma = 8, y_bar = 1.12, n = 5)
```

```{r}
plot_normal_normal(mean = -14, sd = 2, sigma = 8, y_bar = 1.12, n = 5)
```

Comparing the two plots above, we can see the MCMC approximation is very similar to the real posterior model (Both indicate that μ values around -10 are the most likely and the plausible range of μ values is between -16 and approximately -4).